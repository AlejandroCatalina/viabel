{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "VIABEL currently supports both standard KL-based variational inference (KLVI) and chi-squared variational inference (CHIVI). Models are provided as Autograd-compatible log densities or can be constructed from PyStan fit objects.\n",
    "\n",
    "As a simple example, we consider Neal's funnel distribution in 2 dimensions so that we can visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd.scipy.stats.norm as norm\n",
    "\n",
    "D = 2  # number of dimensions\n",
    "log_sigma_stdev = 1. # 1.35\n",
    "def log_density(x):\n",
    "    mu, log_sigma = x[:, 0], x[:, 1]\n",
    "    sigma_density = norm.logpdf(log_sigma, 0, log_sigma_stdev)\n",
    "    mu_density = norm.logpdf(mu, 0, np.exp(log_sigma))\n",
    "    return sigma_density + mu_density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Black-box Variational Inference\n",
    "\n",
    "VIABEL's `bbvi` function provides reasonable defaults: the objective is the ELBO (i.e., the including Kullback-Leibler divergence), a mean-field Gaussian approximation family, and automated RMSProp with adaptive step reduction and stopping rule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from viabel import bbvi, MFStudentT\n",
    "results = bbvi(D, log_density=log_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot contours the from the approximation Gaussian (red) and the target funnel distribution (black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=2, rc={'lines.linewidth': 2})\n",
    "\n",
    "def plot_approx_and_exact_contours(log_density, approx, var_param,\n",
    "                                   xlim, ylim, cmap2='Reds'):\n",
    "    xlist = np.linspace(*xlim, 100)\n",
    "    ylist = np.linspace(*ylim, 100)\n",
    "    X, Y = np.meshgrid(xlist, ylist)\n",
    "    XY = np.concatenate([np.atleast_2d(X.ravel()), np.atleast_2d(Y.ravel())]).T\n",
    "    zs = np.exp(log_density(XY))\n",
    "    Z = zs.reshape(X.shape)\n",
    "    zsapprox = np.exp(approx.log_density(var_param, XY))\n",
    "    Zapprox = zsapprox.reshape(X.shape)\n",
    "    plt.contour(X, Y, Z, cmap='Greys', linestyles='solid')\n",
    "    plt.contour(X, Y, Zapprox, cmap=cmap2, linestyles='solid')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.show()\n",
    "\n",
    "plot_approx_and_exact_contours(log_density, results['objective'].approx, results['var_param'], \n",
    "                               xlim=[-2, 2], ylim=[-3.25, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics\n",
    "\n",
    "VIABEL also has a suite of diagostics for variational inference. We can easily run these using the `vi_diagnostics` function, although low-level support is also provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from viabel import vi_diagnostics\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    diagnostics = vi_diagnostics(results['var_param'], objective=results['objective'], n_samples=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Sampling\n",
    "\n",
    "The Pareto-smoothed weights provide a fairly accuracy approximation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.exp(diagnostics['smoothed_log_weights'])\n",
    "all_samples = diagnostics['samples']\n",
    "\n",
    "# for illustation, sample 1000 weights for visualization\n",
    "subset = np.random.choice(all_samples.shape[1], size=1000, p=weights)\n",
    "samples = all_samples[:,subset]\n",
    "plt.plot(samples[0], samples[1], '*', alpha=.3)\n",
    "plot_approx_and_exact_contours(log_density, results['objective'].approx, results['var_param'], \n",
    "                               xlim=[-2, 2], ylim=[-3.25, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
