{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import multivariate_normal\n",
    "import sys, os\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "from  viabel._distributions import  multivariate_t_logpdf\n",
    "import autograd.scipy.stats.multivariate_normal as mvn\n",
    "import autograd.scipy.stats.multivariate_t as mvn\n",
    "\n",
    "\n",
    "from paragami import (PatternDict,\n",
    "                      NumericVectorPattern,\n",
    "                      PSDSymmetricMatrixPattern,\n",
    "                      FlattenFunctionInput)\n",
    "\n",
    "from viabel.vb import (mean_field_gaussian_variational_family,\n",
    "                       mean_field_t_variational_family,\n",
    "                       t_variational_family,\n",
    "                       black_box_klvi,\n",
    "                       black_box_chivi,\n",
    "                       make_stan_log_density,\n",
    "                       _get_mu_sigma_pattern,\n",
    "                       adagrad_optimize\n",
    "                      )\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook', font_scale=2, rc={'lines.linewidth': 2})\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(means, covs, colors=None, xlim=[-10,10], ylim=[-3, 3], corr=None):\n",
    "    xlist = np.linspace(xlim[0], xlim[1], 100)\n",
    "    ylist = np.linspace(ylim[0], ylim[1], 100)\n",
    "    X,Y = np.meshgrid(xlist, ylist)\n",
    "    XY = np.concatenate([X[:,:,np.newaxis], Y[:,:,np.newaxis]], axis=2)\n",
    "    colors = colors or sns.color_palette()\n",
    "    for m, c, col in zip(means, covs, colors):\n",
    "        Z = multivariate_normal.pdf(XY, mean=m, cov=c)\n",
    "        plt.contour(X, Y, Z, colors=[col], linestyles='solid')\n",
    "    if corr is not None:\n",
    "        plt.title('correlation = {:.2f}'.format(corr))\n",
    "        plt.savefig('../writing/variational-objectives/figures/kl-vb-corr-{:.2f}.pdf'.format(corr), \n",
    "                    bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_approx_and_exact_contours(logdensity, var_family, var_param,colors=None, \n",
    "                                    xlim=[-2.5,2.5], ylim=[-3, 3],\n",
    "                                    savepath=None, aux_var=None):\n",
    "    xlist = np.linspace(*xlim, 100)\n",
    "    ylist = np.linspace(*ylim, 100)\n",
    "    X, Y = np.meshgrid(xlist, ylist)\n",
    "    XY = np.concatenate([np.atleast_2d(X.ravel()), np.atleast_2d(Y.ravel())]).T\n",
    "    if aux_var is not None:\n",
    "        a1= XY.shape[0]\n",
    "        XY = np.concatenate([XY, np.repeat(aux_var[None,:], a1, axis=0)], axis=1)\n",
    "    zs = np.exp(logdensity(XY))\n",
    "    Z = zs.reshape(X.shape)\n",
    "    zsapprox = np.exp(var_family.logdensity(XY, var_param))\n",
    "    Zapprox = zsapprox.reshape(X.shape)\n",
    "    colors = colors or sns.color_palette()\n",
    "    plt.contour(X, Y, Z, colors=[colors[0]], linestyles='solid')\n",
    "    plt.contour(X, Y, Zapprox, colors=[colors[1]], linestyles='solid')\n",
    "    if savepath is not None:\n",
    "        plt.savefig(savepath, bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhos = [.5, .88, .94, .99]\n",
    "\n",
    "def _get_mu_sigma_pattern(dim):\n",
    "    ms_pattern = PatternDict(free_default=True)\n",
    "    ms_pattern['mu'] = NumericVectorPattern(length=dim)\n",
    "    ms_pattern['Sigma'] = PSDSymmetricMatrixPattern(size=dim)\n",
    "    return ms_pattern\n",
    "# rhos.reverse()\n",
    "ds = np.concatenate([np.arange(2,10,2), np.arange(10,105,20,dtype=int)]) # np.arange(2,11,2,dtype=int)\n",
    "df = pd.DataFrame(columns=['corr', 'd', 'KL'])\n",
    "inc_df = pd.DataFrame(columns=['corr', 'd', 'KL'])\n",
    "n_iters = 10000\n",
    "for rho in rhos:\n",
    "    for d in ds:\n",
    "        c2 = rho*np.ones((d,d))\n",
    "        c2[np.diag_indices_from(c2)] = 1\n",
    "        m2 = np.zeros(d)\n",
    "        \n",
    "        def objective(logc1):\n",
    "            c1 = np.diag(np.exp(logc1))\n",
    "            return gaussianKL(m2, c1, m2, c2)\n",
    "\n",
    "        init_log_std = np.ones(d)*0.2   \n",
    "        init_var_param1 = np.concatenate([m2, init_log_std])\n",
    "        mf_t_var_family = mean_field_t_variational_family(d, df=6)\n",
    "        \n",
    "        ms_pattern = _get_mu_sigma_pattern(d)\n",
    "        \n",
    "        lnpdf2 = FlattenFunctionInput(\n",
    "        lambda x: multivariate_t_logpdf(x, m2, c2, 100000),\n",
    "        patterns=ms_pattern, free=True, argnums=1)\n",
    "        lnpdf = lambda z: mvn.logpdf(z, m2, c2)\n",
    "        #lnpdf_t = lambda z:\n",
    "        \n",
    "        klvi_objective_and_grad = black_box_klvi(mf_t_var_family, lnpdf, 2000)\n",
    "        klvi_var_param,  klvi_param_history, value_history, grad_norm_history, oplog = \\\n",
    "        adagrad_optimize(1400, klvi_objective_and_grad, init_var_param1, learning_rate=.02, \n",
    "                                  learning_rate_end=0.001)\n",
    "        if d == 2:\n",
    "            plot_contours(means=[m2]*2, covs=[c2, np.diag(np.exp(res.x))], \n",
    "                          colors=[(0.,0.,0.)]+sns.color_palette(),\n",
    "                          xlim=[-2.5,2.5], corr=rho)\n",
    "        df = df.append(dict(corr=rho, dimension=d, KL=value_history[-1]), ignore_index=True)\n",
    "        #inc_kl = gaussianKL(m2, c2, m2, np.diag(np.exp(klvi_var_param[d:])) )\n",
    "        #inc_df = inc_df.append(dict(corr=rho, dimension=d, KL=inc_kl), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we plot KL-divergence at the KLVI solution for increasing correlation and increasing dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lineplot(data=df, x='corr', y='KL', hue='d', legend='full')\n",
    "# plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
    "# sns.despine()\n",
    "# plt.show()\n",
    "sns.lineplot(data=df, x='dimension', y='KL', hue='corr', legend='full')\n",
    "#plt.legend(rhos, bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
    "plt.ylabel('KL divergence')\n",
    "plt.legend(rhos, loc='upper center', bbox_to_anchor=(0.5, 1.4),\n",
    "           ncol=3, frameon=False)\n",
    "sns.despine()\n",
    "plt.savefig('../writing/variational-objectives/figures/kl-gaussian_mean_t_kl.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rhos = [0.50, .94, .99]\n",
    "# rhos.reverse()\n",
    "ds = np.concatenate([np.arange(2,10,100)]) # np.arange(2,11,2,dtype=int)\n",
    "df = pd.DataFrame(columns=['corr', 'd', 'KL'])\n",
    "inc_df = pd.DataFrame(columns=['corr', 'd', 'KL'])\n",
    "n_iters = 10000\n",
    "for rho in rhos:\n",
    "    for d in ds:\n",
    "        c2 = rho*np.ones((d,d))\n",
    "        c2[np.diag_indices_from(c2)] = 1\n",
    "        m2 = np.zeros(d)\n",
    "        init_log_std = np.ones(d)*0.2   \n",
    "        init_var_param1 = np.concatenate([m2, init_log_std])\n",
    "        mf_t_var_family = mean_field_t_variational_family(d, df=6)\n",
    "        lnpdf = lambda z: mvn.logpdf(z, m2, c2)\n",
    "        #lnpdf_t = lambda z:\n",
    "        \n",
    "        klvi_objective_and_grad = black_box_klvi(mf_t_var_family, lnpdf, 2000)\n",
    "        klvi_var_param,  klvi_param_history, value_history, grad_norm_history, oplog = \\\n",
    "        adagrad_optimize(2000, klvi_objective_and_grad, init_var_param1, learning_rate=.02, \n",
    "                                  learning_rate_end=0.001)\n",
    "        if d == 2:\n",
    "            plot_approx_and_exact_contours(lnpdf, mf_t_var_family, klvi_var_param, colors=[(0.,0.,0.)]+sns.color_palette())\n",
    "        df = df.append(dict(corr=rho, dimension=d, KL=value_history[-1]), ignore_index=True)\n",
    "        #inc_kl = gaussianKL(m2, c2, m2, np.diag(np.exp(klvi_var_param[d:])) )\n",
    "        #inc_df = inc_df.append(dict(corr=rho, dimension=d, KL=inc_kl), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we plot inclusive KL-divergence at the KLVI solution for increasing correlation and increasing dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=inc_df, x='dimension', y='KL', hue='corr', legend='full')\n",
    "#plt.legend(rhos, bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
    "plt.ylabel('Inclusive KL divergence')\n",
    "plt.legend(rhos, loc='upper center', bbox_to_anchor=(0.5, 1.4),\n",
    "           ncol=3, frameon=False)\n",
    "sns.despine()\n",
    "plt.savefig('../writing/variational-objectives/figures/inckl-vb-d.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now at inclusive KLVI solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_inc_df = pd.DataFrame(columns=['corr', 'd', 'KL'])\n",
    "inc_inc_df = pd.DataFrame(columns=['corr', 'd', 'KL'])\n",
    "for rho in rhos:\n",
    "    for d in ds:\n",
    "        c2 = rho*np.ones((d,d))\n",
    "        c2[np.diag_indices_from(c2)] = 1\n",
    "        m2 = np.zeros(d)\n",
    "        def objective(logc1):\n",
    "            c1 = np.diag(np.exp(logc1))\n",
    "            return gaussianKL(m2, c2, m2, c1)\n",
    "        res = minimize(objective, np.ones(d)*0.4, method='BFGS', jac=grad(objective))\n",
    "        if d == 2:\n",
    "            plot_contours(means=[m2]*2, covs=[c2, np.diag(np.exp(res.x))], \n",
    "                          colors=[(0.,0.,0.)]+sns.color_palette(),\n",
    "                          xlim=[-2.5,2.5], corr=rho)\n",
    "        inc_inc_df = inc_inc_df.append(dict(corr=rho, d=d, KL=res.fun), ignore_index=True)\n",
    "        kl = gaussianKL( m2, np.diag(np.exp(res.x)), m2, c2 )\n",
    "        kl_inc_df = kl_inc_df.append(dict(corr=rho, d=d, KL=kl), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we plot inclusive KL-divergence at the inclusive KLVI solution for increasing correlation and increasing dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=inc_inc_df, x='d', y='KL', hue='corr', legend='full')\n",
    "#plt.legend(rhos, bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
    "plt.ylabel('Inclusive KL divergence')\n",
    "plt.legend(rhos, loc='upper center', bbox_to_anchor=(0.5, 1.4),\n",
    "           ncol=3, frameon=False)\n",
    "sns.despine()\n",
    "plt.savefig('../writing/variational-objectives/figures/inc_kl_soln_inckl-vb-d.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we plot KL-divergence at the inclusive KLVI solution for increasing correlation and increasing dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=kl_inc_df, x='d', y='KL', hue='corr', legend='full')\n",
    "#plt.legend(rhos, bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
    "plt.ylabel('KL divergence')\n",
    "plt.legend(rhos, loc='upper center', bbox_to_anchor=(0.5, 1.4),\n",
    "           ncol=3, frameon=False)\n",
    "sns.despine()\n",
    "plt.savefig('../writing/variational-objectives/figures/kl_at_inckl-vb-d.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inc_inc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kl_inc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kl_inc_df.d[125], kl_inc_df.KL[125])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (viabel_env)",
   "language": "python",
   "name": "viabel_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
